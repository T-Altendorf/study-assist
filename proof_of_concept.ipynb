{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from key import api_key\n",
    "\n",
    "# Ensure you've set up your API keys properly\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)  # Corrected this line\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"lectures\\\\04_baumbasierteVerfahren.pdf\"\n",
    "pdf_text = extract_text_from_pdf(file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def break_into_sections(text, max_tokens_per_section):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    words = text.split()\n",
    "    sections = []\n",
    "    section = \"\"\n",
    "    token_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        if token_count + len(tokens) < max_tokens_per_section:\n",
    "            section += \" \" + word\n",
    "            token_count += len(tokens)\n",
    "        else:\n",
    "            sections.append(section.strip())\n",
    "            section = word\n",
    "            token_count = len(tokens)\n",
    "    \n",
    "    # Add the last section if it's non-empty\n",
    "    if section:\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Example usage:\n",
    "sections = break_into_sections(pdf_text, max_tokens_per_section=1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def summarize_text(text):\n",
    "    sections = break_into_sections(text, max_tokens_per_section=1700)\n",
    "    summaries = []\n",
    "    \n",
    "    for section in sections:\n",
    "        summary = \"\"\n",
    "        while not summary:\n",
    "            try:\n",
    "                response = openai.Completion.create(\n",
    "                    engine=\"gpt-3.5-turbo-instruct\",\n",
    "                    prompt=f\"{section} \\n\\n This part can be summarized as follows:\",\n",
    "                    max_tokens=200\n",
    "                )\n",
    "                summary = response['choices'][0]['text'].strip()\n",
    "                summaries.append(summary)\n",
    "            except openai.error.OpenAIError as e:\n",
    "                if 'Rate limit reached' in str(e):\n",
    "                    print('Rate limit exceeded. Waiting for 60 seconds...')\n",
    "                    time.sleep(30)  # Wait for 60 seconds before retrying\n",
    "                else:\n",
    "                    print(f'An error occurred: {e}')\n",
    "                    break  # Break out of the loop for other errors\n",
    "        print(f'Summarized {len(summaries)} out of {len(sections)} sections')\n",
    "        print(f'Current summary: {summary}')\n",
    "    \n",
    "    return \" \".join(summaries)\n",
    "\n",
    "final_summary = summarize_text(pdf_text)\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_final(summary):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=f\"{summary} \\n \\n Overall one can summarize the text in the following 10 bullets each having subpoints to elaborate further:\",\n",
    "        max_tokens=600\n",
    "    )\n",
    "    print(response[\"choices\"])\n",
    "    return response['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullets = summarize_final(final_summary)\n",
    "print(bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=f\"Summarize the following text:\\n{text}\",\n",
    "        max_tokens=300  # Limit the summary length\n",
    "    )\n",
    "    summary = response['choices'][0]['text'].strip()\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "summary = summarize_text(pdf_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Mining und Maschinelles Lernen\n",
      "Baumbasierte Verfahren\n",
      "Entscheidungsbäume sind auf den ersten Blick ein grundlegend anderer Ansatz\n",
      "zur Konstruktion von Klassiﬁkationsregeln. Ihre Regeln sind meist sehr verständlich\n",
      "fur den Anwender, können sich aber nicht sehr ﬂexibel an die Daten anpassen. In\n",
      "R kann z.B. rpart() aus dem Paket rpart benutzt werden.\n",
      "Basierend auf Folien von Katharina Morik, Uwe Ligges, Christoph Sawade, Niels Landwehr, Paul Prasse, Silvia Makowski, Tobias Scheffer und Johannes Fürnkranz.\n",
      "Danke fürs Offenlegen der Folien.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 1\n",
      "Unsere Ziele\n",
      "Was wollen wir hier kennenlernen?\n",
      "▶ Was sind Entscheidungsbäume?\n",
      "▶ Was ist der Informationsgewinn von Tests?\n",
      "▶ Wie lernen wir Entscheidungsbäume?\n",
      "▶ Wie vermeiden wir “Wildwuchs” bei Entscheidungsbäumen?\n",
      "▶ Gütekosten\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 2\n",
      "Aufteilen der Beispiele und Modellierung jeder\n",
      "Region\n",
      "▶ Globale Modelle: Lineare Modelle passen die Parameter eines Modells für den\n",
      "gesamten Merkmalsraum der Beispiele an, der evtl. durch eine implizite\n",
      "Transformation (Kernfunktionen, dazu später mehr) oder explizite\n",
      "Transformationen (Vorverarbeitung) in einen Merkmalsraum überführt werden.\n",
      "▶ Lokale Modelle: k-nächste Nachbarn (kNN) Verfahren, die wir auch schon\n",
      "kennengelernen haben, teilen den Raum der Beispiele bei einer Anfrage x in\n",
      "die Nachbarschaft von x und den Rest auf.\n",
      "▶ Partitionierte Modelle: Baumlerner teilen den gesamten Merkmalsraum in\n",
      "Rechtecke auf und passen in jedem ein Modell an. Dabei wird die Wahl des\n",
      "Merkmals in der rekursiven Aufteilung automatisch bestimmt.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 3\n",
      "Entscheidungsbäume\n",
      "Ein Beispiel aus der Produktion\n",
      "Stanzen von Löchern in Abhängigkeit des\n",
      "Typs und der Dicke des Materials. (Links)\n",
      "Produktionsteil. (Rechts) Sensorwerte.\n",
      "Vorhersage des Materials\n",
      "Danke an das Institut für Produktionstechnik und Umformmaschinen der TU Darmstadt für die Daten!\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 4\n",
      "Entscheidungsbäume\n",
      "Ein weiteres Beispiel aus der Kreditwirtschaft\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 5\n",
      "Warum Entscheidungsbäume?\n",
      "▶ Bäume sind (recht) einfach zu\n",
      "interpretieren\n",
      "▶ Sie liefern direkt Vorhersagen und\n",
      "Begründungen: “Abgelehnt, weil weniger\n",
      "als 3 Monate beschäftigt und\n",
      "Kredit-Sicherheiten <2x verfügbares\n",
      "Einkommen”.\n",
      "▶ Bäume können aus Beispielen gelernt werden: Einfache Lernalgorithmen,\n",
      "die efﬁzient und skalierbar ist.\n",
      "▶ Es gibt Bäume sowohl für Klassiﬁkation und Regression.\n",
      "▶ Und sie lassen sich mit Modellen wie z.B. linearen Modellen kombinieren.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 6\n",
      "Was sind denn nun Entscheidungsbäume?\n",
      "Entscheidungsbaum\n",
      "(n-äre) Entscheidungsbäume bestehen\n",
      "aus einer Abfolge von n Entscheidungen\n",
      "(Test), die als Baum darstellbar sind.\n",
      "▶ Bei binären Bäumen ﬁndet in jedem Innerenknoten eine “Ja-Nein” (binäre)\n",
      "Entscheidung statt. Bei “Ja” folgt die Entscheidung im nächsten Knoten auf\n",
      "der linken Seite, bei “Nein” auf der rechten Seite.\n",
      "▶ Die Entscheidungen werden so getroffen, dass die Knoten möglichst eine\n",
      "“reine” Klasse darstellen.\n",
      "▶ In jedem Terminalknoten (Blatt) wird eine Zuordnung zu der Klasse getroffen,\n",
      "die dort am häuﬁgsten vorkommt.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 7\n",
      "Klassiﬁzieren mit Entscheidungsbäumen\n",
      "Ein weiteres Beispiel aus der landwirtschaftli-\n",
      "chen Produktion\n",
      "Die Bodenprobe für Rotbuchen ist\n",
      "(trocken,alkalisch,7) und wird als geeig-\n",
      "net klassiﬁziert (“+”)\n",
      "Feuchte\n",
      "Säure\n",
      "Temp\n",
      "-\n",
      "≤ 3, 5\n",
      "+\n",
      "> 3, 5\n",
      "basisch\n",
      "Temp\n",
      "+\n",
      "≤ 7, 5\n",
      "-\n",
      "> 7, 5\n",
      "neutral\n",
      "+\n",
      "alkalisch\n",
      "trocken\n",
      "Temp\n",
      "-\n",
      "≤ 9\n",
      "+\n",
      "> 9\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 8\n",
      "Lernen aus Beispielen\n",
      "+\n",
      "-\n",
      "ID\n",
      "Feuchte\n",
      "Säure\n",
      "Temp\n",
      "ID\n",
      "Feuchte\n",
      "Säure\n",
      "Temp\n",
      "1\n",
      "trocken\n",
      "basisch\n",
      "7\n",
      "2\n",
      "feucht\n",
      "neutral\n",
      "8\n",
      "3\n",
      "trocken\n",
      "neutral\n",
      "7\n",
      "4\n",
      "feucht\n",
      "alkal.\n",
      "5\n",
      "6\n",
      "trocken\n",
      "neutral\n",
      "6\n",
      "5\n",
      "trocken\n",
      "neutral\n",
      "8\n",
      "9\n",
      "trocken\n",
      "alkal.\n",
      "9\n",
      "7\n",
      "trocken\n",
      "neutral\n",
      "11\n",
      "10\n",
      "trocken\n",
      "alkal.\n",
      "8\n",
      "8\n",
      "trocken\n",
      "neutral\n",
      "9\n",
      "12\n",
      "feucht\n",
      "neutral\n",
      "10\n",
      "11\n",
      "feucht\n",
      "basisch\n",
      "7\n",
      "13\n",
      "trocken\n",
      "basisch\n",
      "6\n",
      "14\n",
      "feucht\n",
      "alkal.\n",
      "7\n",
      "16\n",
      "trocken\n",
      "basisch\n",
      "4\n",
      "15\n",
      "trocken\n",
      "basisch\n",
      "3\n",
      "Ohne einen Test zu benutzten, können wir als Vorhersage immer “-” sagen\n",
      "(Majority Vote, hier mit Tie-Break für “-”). Der Fehler ist dann 8/16.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 9\n",
      "Aufteilen nach Bodenfeuchte\n",
      "Ein einzelner Test verbessert die Vorhersage!\n",
      "Vorhersage der häuﬁgsten Klasse in jedem Blatt:\n",
      "11\n",
      "16-mal trocken, Majority Vote “+”: Fehler\n",
      "4\n",
      "11\n",
      "5\n",
      "16-mal feucht, Majority Vote “-”: Fehler 1\n",
      "5\n",
      "Fehler wenn wir die Information über Feuchte\n",
      "berücksichtigen:\n",
      "11\n",
      "16 · 4\n",
      "11 + 5\n",
      "16 · 1\n",
      "5 =\n",
      "5\n",
      "16, was kleiner als\n",
      "8\n",
      "16 ist!\n",
      "Feuchte\n",
      "1 basisch 7 +\n",
      "3 neutral 7 +\n",
      "5 neutral 8 -\n",
      "6 neutral 6 +\n",
      "7 neutral 11 -\n",
      "8 neutral 9 -\n",
      "9 alkal.9 +\n",
      "10 alkal. 8 +\n",
      "13 basisch 6 +\n",
      "15 basisch 3 -\n",
      "16 basisch 4 +\n",
      "trocken\n",
      "2 neutral 8 -\n",
      "4 alkal. 5 -\n",
      "11 basisch 7 -\n",
      "12 neutral 10+\n",
      "14 alkal. 7 -\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 10\n",
      "Wir\n",
      "haben\n",
      "bedingte\n",
      "Wahrscheinlichkeiten\n",
      "benutzt\n",
      "Bedingte Wahrscheinlichkeit\n",
      "Wahrscheinlichkeit, dass ein Beispiel zu einer Klasse gehört, gegeben der\n",
      "Merkmalswert\n",
      "P(Y|Xj) = P(Y ∩ Xj)/P(Xj)\n",
      "▶ Wir kennen die echte Verteilung nicht! Daher Annäherung der\n",
      "Wahrscheinlichkeit über die Häuﬁgkeit in den Lernbeispielen mit Gewichtung\n",
      "bezüglich der Oberklasse. Beispiel: Y = {+, −}, Xj = {feucht, trocken}\n",
      "P(+|feucht) = 1/5, P(−|feucht) = 4/5 gewichtet mit 5/16\n",
      "P(+|trocken) = 7/11, P(−|trocken) = 4/11 gewichtet mit 11/16\n",
      "▶ Wahl des Merkmals mit dem höchsten Wert (kleinsten Fehler)\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 11\n",
      "Eine erste Idee: Betrachte den Informationsge-\n",
      "winn eines Merkmals\n",
      "Information\n",
      "Sei p+ die Wahrscheinlichkeit, dass ein Beispiel der Klasse “+” entstammt. Damit\n",
      "können wir die Entropy berechnen:\n",
      "I(p+, p−) = (−p+ log p+) + (−p− log p−)\n",
      "Ein Merkmal Xj mit k Werten teilt eine Menge von Beispielen X in k Untermengen\n",
      "X1, ..., Xk auf. Für jede dieser Mengen berechnen wir die Entropie.\n",
      "Information(Xj, X) := −\n",
      "�k\n",
      "i=1\n",
      "|Xi|\n",
      "|X| I(p+, p−)\n",
      "Informationsgewinn\n",
      "Differenz zwischen den Informationen der Beispiele mit und ohne die Aufteilung\n",
      "durch Xj.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 12\n",
      "Informationsgewinn des Merkmals Feuchte\n",
      "Güte des Attributs Feuchte mit den 2 Werten trocken\n",
      "und feucht:\n",
      "−\n",
      "\n",
      "\n",
      "11\n",
      "16 · I(+, −)\n",
      "�\n",
      "��\n",
      "�\n",
      "trocken\n",
      "+\n",
      "5\n",
      "16 · I(+, −)\n",
      "�\n",
      "��\n",
      "�\n",
      "feucht\n",
      "\n",
      "\n",
      "=\n",
      "−\n",
      "\n",
      "\n",
      "11\n",
      "16 ·\n",
      "�\n",
      "− 7\n",
      "11 · log\n",
      "�\n",
      "7\n",
      "11\n",
      "�\n",
      "− 4\n",
      "11 · log\n",
      "�\n",
      "4\n",
      "11\n",
      "��\n",
      "�\n",
      "��\n",
      "�\n",
      "trocken\n",
      "+ 5\n",
      "16\n",
      "�\n",
      "−1\n",
      "5 · log\n",
      "�\n",
      "1\n",
      "5\n",
      "�\n",
      "− 4\n",
      "5 · log\n",
      "�\n",
      "4\n",
      "5\n",
      "��\n",
      "�\n",
      "��\n",
      "�\n",
      "feucht\n",
      "\n",
      " = −0, 27\n",
      "alle 16 Beispiele\n",
      "11 Beispiele:\n",
      "7 davon +\n",
      "4 davon -\n",
      "trocken\n",
      "5 Beispiele:\n",
      "1 davon +\n",
      "4 davon -\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 13\n",
      "Informationsgewinn des Merkmals Säure\n",
      "Güte des Attributs Säure mit den 3 Werten basisch,\n",
      "neutral und alkalisch:\n",
      "−\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "16 · I(+, −)\n",
      "�\n",
      "��\n",
      "�\n",
      "basisch\n",
      "+ 7\n",
      "16 · I(+, −)\n",
      "�\n",
      "��\n",
      "�\n",
      "neutral\n",
      "+ 4\n",
      "16 · I(+, −))\n",
      "�\n",
      "��\n",
      "�\n",
      "alkalisch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "= −0, 3\n",
      "basisch − 3\n",
      "5 · log\n",
      "� 3\n",
      "5\n",
      "�\n",
      "+ − 2\n",
      "5 · log\n",
      "� 2\n",
      "5\n",
      "�\n",
      "neutral − 3\n",
      "7 · log\n",
      "� 3\n",
      "7\n",
      "�\n",
      "+ − 4\n",
      "7 · log\n",
      "� 4\n",
      "7\n",
      "�\n",
      "alkalisch − 2\n",
      "4 · log\n",
      "� 2\n",
      "4\n",
      "�\n",
      "+ − 2\n",
      "4 · log\n",
      "� 2\n",
      "4\n",
      "�\n",
      "alle 16 Beispiele\n",
      "basisch\n",
      "3 davon +\n",
      "2 davon -\n",
      "neutral\n",
      "3 davon +\n",
      "4 davon -\n",
      "alkalisch\n",
      "2 davon +\n",
      "2 davon -\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 14\n",
      "Informationsgewinn des Merkmals Temperatur\n",
      "▶ Die Temperatur ist ein numerischer Wert! Es gibt also unendlich viele\n",
      "Temperaturwerte\n",
      "▶ Einfachste Lösung: Numerische Merkmalswerte werden nach Schwellwerten\n",
      "eingeteilt.\n",
      "▶ 9 verschiedene Werte in der Beispielmenge, also 8 Möglichkeiten zu trennen.\n",
      "▶ Wert mit der kleinsten Fehlerrate bei Vorhersage der Mehrheitsklasse liegt bei 7.\n",
      "▶ 5 Beispiele mit Temp ≤ 7, davon 3 in +,\n",
      "11 Beispiele Temp > 7, davon 6 in -.\n",
      "▶ Die Güte (also hier der Informationsgewinn) der Temperatur als Merkmal ist\n",
      "dann −0, 29.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 15\n",
      "Mit dem Informationsgehalt können wir\n",
      "Merkmale als Tests auswählen\n",
      "▶ Wir wählen das Merkmal Xj, dessen Werte am besten in (Unter-)mengen Xi\n",
      "aufteilen, die geordnet sind.\n",
      "▶ Das Gütekriterium Informationsgewinn (mittels Entropie) bestimmt die\n",
      "Ordnung der Mengen.\n",
      "▶ In unserem Beispiel hat Feuchte den höchsten Gütewert.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 16\n",
      "Alg.: Top Down Induction of Decision Trees\n",
      "(TDIDT, hier: ID3) am Beispiel\n",
      "Feuchte\n",
      "1 basisch 7 +\n",
      "3 neutral 7 +\n",
      "5 neutral 8 -\n",
      "6 neutral 6 +\n",
      "7 neutral 11 -\n",
      "8 neutral 9 -\n",
      "9 alkal.9 +\n",
      "10 alkal. 8 +\n",
      "13 basisch 6 +\n",
      "15 basisch 3 -\n",
      "16 basisch 4 +\n",
      "trocken\n",
      "2 neutral 8 -\n",
      "4 alkal. 5 -\n",
      "11 basisch 7 -\n",
      "12 neutral 10 +\n",
      "14 alkal. 7 -\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 17\n",
      "Algorithmus TDIDT (ID3) am Beispiel\n",
      "Feuchte\n",
      "Säure\n",
      "1 basisch 7 +\n",
      "13 basisch 6 +\n",
      "15 basisch 3 -\n",
      "16 basisch 4 +\n",
      "basisch\n",
      "3 neutral 7 +\n",
      "5 neutral 8 -\n",
      "6 neutral 6 +\n",
      "7 neutral 11 -\n",
      "8 neutral 9 -\n",
      "neutral\n",
      "9 alkal. 9 +\n",
      "10 alkal. 8 +\n",
      "alkalisch\n",
      "trocken\n",
      "2 neutral 8 -\n",
      "4 alkal. 5 -\n",
      "11 basisch 7 -\n",
      "12 neutral 10 +\n",
      "14 alkal. 7 -\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 18\n",
      "Algorithmus TDIDT (ID3) am Beispiel\n",
      "Feuchte\n",
      "Säure\n",
      "1 basisch 7 +\n",
      "13 basisch 6 +\n",
      "15 basisch 3 -\n",
      "16 basisch 4 +\n",
      "basisch\n",
      "Temp\n",
      "3 neutral 7 +\n",
      "6 neutral 6 +\n",
      "≤ 7.5\n",
      "5 neutral 8 -\n",
      "7 neutral 11 -\n",
      "8 neutral 9 -\n",
      "> 7.5\n",
      "neutral\n",
      "9 alkal. 9 +\n",
      "10 alkal. 8 +\n",
      "alkalisch\n",
      "trocken\n",
      "2 neutral 8 -\n",
      "4 alkal. 5 -\n",
      "11 basisch 7 -\n",
      "12 neutral 10 +\n",
      "14 alkal. 7 -\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 19\n",
      "Algorithmus TDIDT (ID3) am Beispiel\n",
      "Feuchte\n",
      "Säure\n",
      "Temp\n",
      "15 basisch 3 -\n",
      "≤ 3.5\n",
      "1 basisch 7 +\n",
      "13 basisch 6 +\n",
      "16 basisch 4 +\n",
      "> 3.5\n",
      "basisch\n",
      "Temp\n",
      "3 neutral 7 +\n",
      "6 neutral 6 +\n",
      "≤ 7.5\n",
      "5 neutral 8 -\n",
      "7 neutral 11 -\n",
      "8 neutral 9 -\n",
      "> 7.5\n",
      "neutral\n",
      "9 alkal. 9 +\n",
      "10 alkal. 8 +\n",
      "alkalisch\n",
      "trocken\n",
      "2 neutral 8 -\n",
      "4 alkal. 5 -\n",
      "11 basisch 7 -\n",
      "12 neutral 10 +\n",
      "14 alkal. 7 -\n",
      "feucht\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 20\n",
      "Algorithmus ID3 (TDIDT)\n",
      "Rekursive Aufteilung der Beispielmenge nach Merkmalsauswahl:\n",
      "1. TDIDT(X, {X1, ...Xp})\n",
      "2. X enthält nur Beispiele einer Klasse → fertig\n",
      "3. X enthält Beispiele verschiedener Klassen:\n",
      "▶ G ¨ute(X1, .., Xp, X)\n",
      "▶ Wahl des besten Merkmals Xj mit k Werten\n",
      "▶ Aufteilung von X in X1, X2, ..., Xk\n",
      "▶ für i = 1, ..., k:\n",
      "TDIDT(Xi, {X1, ..., Xp}\\Xj)\n",
      "▶ Resultat ist aktueller Knoten mit den Teilbäumen T1, ..., Tk\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 21\n",
      "Komplexität TDIDT ohne Pruning\n",
      "(Stutzen des Baumes)\n",
      "Das Lernen von Enscheidungsbäumen ist efﬁzient und skaliert gut.\n",
      "Rekursive Aufteilung der Beispielmenge nach Merkmalsauswahl:\n",
      "▶ Bei p (nicht-numerischen) Merkmalen und N Beispielen ist die Komplexität\n",
      "O(pN log N)\n",
      "▶ Die Tiefe des Baums sei in O(log N).\n",
      "▶ O(N log N) alle Beispiele müssen “in die Tiefe verteilt” werden, also: O(N log N)\n",
      "für ein Merkmal.\n",
      "▶ p mal bei p Merkmalen!\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 22\n",
      "Stutzen (pruning) des Baumes\n",
      "Aber Achtung! Es können große Bäume entstehen1. Grosse Bäume können zwei\n",
      "Probleme haben:\n",
      "▶ Obgleich sie sehr genau sind, erzeugen sie grosse Fehlerraten auf neuen\n",
      "Datensätzen, und\n",
      "▶ das Verstehen und Interpretieren von Bäumen mit vielen Terminalknoten ist\n",
      "kompliziert. Grosse Bäume sind also komplexe Bäume.\n",
      "Zwei Extreme: Nur ein Terminalknoten vs. grösster Baum, der möglich ist.\n",
      "Die “richtigen Grösse” sollte zwischen den Extremen liegen.\n",
      "1Die Komplexität eines Baumes wird gemessen durch die Anzahl seiner Terminalknoten.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 23\n",
      "Stutzen (pruning) des Baumes\n",
      "▶ Die Ziele des Stutzens:\n",
      "▶ Überanpassung (Overﬁtting) des\n",
      "Baums an die Trainingsdaten\n",
      "verringern!\n",
      "▶ Verständlichkeit erhöhen!\n",
      "▶ Operationen des Stutzens (Pruning):\n",
      "a) Knoten an Stelle eines Teilbaums\n",
      "setzen\n",
      "b) Einen Teilbaum eine Ebene höher\n",
      "ziehen\n",
      "▶ Schätzen, wie sich der wahre Fehler\n",
      "beim Stutzen entwickelt.\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "a)\n",
      "Knoten an Stelle eines Teilbaums setzen\n",
      "A\n",
      "B\n",
      "E\n",
      "b)\n",
      "Einen Teilbaum eine Ebene höher ziehen\n",
      "A\n",
      "C\n",
      "E\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 24\n",
      "Stutzen durch Fehlerschätzen\n",
      "Die Suche nach dem Baum der “richtigen Grösse” beginnt mit dem Stutzen\n",
      "(pruning) der Äste des grössten Baumes von den Endknoten her (“bottom up”).\n",
      "▶ Wenn der Fehler eines Knotens kleiner ist als die Summe der Fehler seiner\n",
      "Unterknoten, können die Unterknoten weggestutzt werden. Dazu müssen wir\n",
      "(bottom-up) die Fehler an allen Knoten schätzen.\n",
      "▶ Obendrein sollten wir berücksichtigen, wie genau unsere Schätzung ist. Dazu\n",
      "bestimmen wir ein Konﬁdenzintervall: Wenn die obere Schranke der Konﬁdenz\n",
      "in den Fehler beim oberen Knoten kleiner ist als bei allen Unterknoten\n",
      "zusammen, werden die Unterknoten gestutzt.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 25\n",
      "Was ist ein Konﬁdenzintervall?\n",
      "Konﬁdenzintervall\n",
      "Vorgegeben eine tolerierte Irrtumswahrscheinlichkeit α, gibt das Konﬁdenzintervall\n",
      "P(u ≤ X ≤ o) = 1 − α\n",
      "an, dass X mit der Wahrscheinlichkeit 1 − α im Intervall [u, o] liegt und mit der\n",
      "Wahrscheinlichkeit α nicht in [u, o] liegt.\n",
      "Meist wird das Konﬁdenzintervall für den Erwartungswert gebildet. Beispiel\n",
      "α = 0, 1: Mit 90% iger Wahrscheinlichkeit liegt der Mittelwert ¯X im Intervall [u, o],\n",
      "nur 10% der Beobachtungen liefern einen Wert außerhalb des Intervalls.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 26\n",
      "z-Transformation\n",
      "in\n",
      "eine\n",
      "standard-\n",
      "normalverteilte Zufallsvariable\n",
      "Die Zufallsvariable X wird bezüglich ihres Mittelwerts ¯X standardisiert unter der\n",
      "Annahme einer Normalverteilung:\n",
      "Z =\n",
      "¯X − µ\n",
      "σ\n",
      "√\n",
      "N\n",
      "∼ N (0; 1)\n",
      "Die Wahrscheinlichkeit dafür, dass der Mittelwert im Intervall liegt, ist nun:\n",
      "P\n",
      "�\n",
      "−z\n",
      "�\n",
      "1 − α\n",
      "2\n",
      "�\n",
      "≤\n",
      "¯X − µ\n",
      "σ\n",
      "√\n",
      "N\n",
      "≤ z\n",
      "�\n",
      "1 − α\n",
      "2\n",
      "��\n",
      "= 1 − α\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 27\n",
      "Verteilung mit z-Werten\n",
      "Fläche unter der Glocke in [−z, z] = c\n",
      "▶ P(−z ≤ X ≤ z) = 1 − α Konﬁdenzniveau\n",
      "Wahrscheinlichkeit, dass X mit Mittelwert 0 im Intervall der Breite 2z liegt ist\n",
      "1 − α.\n",
      "▶ z kann nachgeschlagen werden (z.B. Bronstein), wobei wegen Symmetrie nur\n",
      "angegeben ist: P(X ≥ z)\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 28\n",
      "Rechnung für reellwertige Beobachtungen und\n",
      "Mittelwert\n",
      "Wir wollen ein bestimmtes Konﬁdenzniveau erreichen, z.B. 0,8.\n",
      "▶ P(X ≥ −z) bzw. P(X ≤ z) ist dann (1 − 0, 8)/2 = 0, 1.\n",
      "▶ Der z-Wert, für den die Fläche der Glockenkurve zwischen −z und z genau\n",
      "1 − α = 0, 8 beträgt, ist das (1 − α\n",
      "2 )-Quantil der Standardnormalverteilung,\n",
      "hier: 1, 28 (nachschlagen).\n",
      "▶ Das standardisierte Stichprobenmittel liegt mit der Wahrscheinlichkeit 0,8\n",
      "zwischen -1,28 und +1,28.\n",
      "0, 8\n",
      "=\n",
      "P(−1, 28 ≤\n",
      "¯X − µ\n",
      "σ\n",
      "√\n",
      "N\n",
      "≤ 1, 28)\n",
      "=\n",
      "P(−1, 28 σ\n",
      "√\n",
      "N\n",
      "≤ ¯X − µ ≤ 1, 28 σ\n",
      "√\n",
      "N\n",
      ")\n",
      "=\n",
      "P( ¯X − 1, 28 σ\n",
      "√\n",
      "N\n",
      "≤ µ ≤ ¯X − 1, 28 σ\n",
      "√\n",
      "N\n",
      ")\n",
      "Das Intervall ist [ ¯X − 1, 28 σ\n",
      "√\n",
      "N ; ¯X + 1, 28 σ\n",
      "√\n",
      "N ].\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 29\n",
      "Fehler bzw. Erfolg schätzen\n",
      "▶ Bei den Entscheidungsbäumen beobachten wir nur zwei Werte Y ∈ {+, −}.\n",
      "▶ Wir haben eine Binomialverteilung mit wahrer Wahrscheinlichkeit\n",
      "p+ für y = + (Erfolg).\n",
      "▶ Beobachtung der Häuﬁgkeit f+ bei N Versuchen.\n",
      "Varianz:\n",
      "σ2 = f+(1 − f+)\n",
      "N\n",
      "Erwartungswert:\n",
      "E(p+) = f+/N\n",
      "▶ In das allgemeine Konﬁdenzintervall [ ¯X − z(1 − α/2) σ\n",
      "√\n",
      "N ; ¯X + 1, 28 σ\n",
      "√\n",
      "N ] setzen\n",
      "wir diese Varianz ein und erhalten:\n",
      "�\n",
      "f+ − z(1 − α/2)\n",
      "√f+(1 − f+)\n",
      "N\n",
      "; f+z(1 − α/2)\n",
      "√f+(1 − f+)\n",
      "N\n",
      "�\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 30\n",
      "Konﬁdenz bei Binomialverteilung\n",
      "Allgemein berechnet man die obere und untere Schranke der Konﬁdenz bei einer\n",
      "Binomialverteilung für ein Bernoulli-Experiment:\n",
      "p+ =\n",
      "f+ + z2\n",
      "2N ± z\n",
      "�\n",
      "f+\n",
      "N − f 2\n",
      "N +\n",
      "z2\n",
      "4N2\n",
      "1 + z2\n",
      "N\n",
      "Hierzu muss lediglich die Häuﬁgkeit f+ gezählt werden, N, z bekannt sein.\n",
      "Diese Abschätzung für den Erfolg können wir symmetrisch für den Fehler (p−)\n",
      "durchführen.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 31\n",
      "Anwendung zum Stutzen\n",
      "▶ Für jeden Knoten nehmen wir die obere Schranke (pessimistisch):\n",
      "p− =\n",
      "f− + z2\n",
      "2N + z\n",
      "�\n",
      "f−\n",
      "N −\n",
      "f 2\n",
      "−\n",
      "N +\n",
      "z2\n",
      "4N2\n",
      "1 + z2\n",
      "N\n",
      "▶ Wenn der Schätzfehler eines Knotens kleiner ist als die Kombination der\n",
      "Schätzfehler seiner Unterknoten, werden die Unterknoten weggestutzt. Die\n",
      "Kombination wird gewichtet mit der Anzahl der subsumierten Beispiele.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 32\n",
      "Andere Gütemaße: Gini-Index\n",
      "Wir wollen messen, wie “rein” die Klasse ist, die ein Knoten darstellt. Neben dem\n",
      "Informationsgewinn gibt es viele andere Maße.\n",
      "Gini-Index\n",
      "Das Gini-Index eines Knoten t ist deﬁniert als\n",
      "I(t) := 1 − S mit S =\n",
      "�k\n",
      "j=1 p2(j|t)\n",
      "wobei S die Reinheitsfunktion ist.\n",
      "▶ CART, ein anderer Baumlerner, nimmt im Wesentlichen den Gini-Index.\n",
      "▶ Der Gini-Index nimmt sein Maximum an, wenn jede Klasse in dem Knoten mit\n",
      "gleicher Wahrscheinlichkeit angenommen wird.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 33\n",
      "Andere Gütemaße: Gini-Index\n",
      "Dazu passen wir dann unser Gütemaß an:\n",
      "Allgemeines Gütemaß\n",
      "Sein s eine Teilung im Knoten t. Das Gütemaß dieser Teiling mißt die Verringerung\n",
      "der “Unreinheit”:\n",
      "∆I(s, t) = I(t) − p+ · I(t+) − p− · I(t−)\n",
      "wobei I(t+) das Gütemaß des linken und I(t−) das des rechten Teilbaums ist.\n",
      "▶ CART, ein anderer Baumlerner, nimmt im Wesentlichen den Gini-Index.\n",
      "▶ Der Gini-Index nimmt sein Maximum an, wenn jede Klasse in dem Knoten mit\n",
      "gleicher Wahrscheinlichkeit angenommen wird.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 34\n",
      "Andere Gütemaße: Regression\n",
      "Unser Ziel sind jetzt niedrige Fehlerquadrate (SSE). Sei t ein Knoten.\n",
      "SSE(t) =\n",
      "�\n",
      "(x,y)∈t\n",
      "(y − ¯y)2 mit ¯y = 1\n",
      "|t|\n",
      "�\n",
      "(x,y)∈t\n",
      "y\n",
      "SSE-Reduktion für [x ≤ z] Tests\n",
      "Seien t+ und − die Auteilung durch [x ≤ z]. Analog zum allgemeinen Gütemaß,\n",
      "berechnen wir\n",
      "∆SSE(s, t) = SSE(t) − SSE(t+) − SSE(t−)\n",
      "▶ Führe keinen neuen Split ein, wenn SSE nicht um Mindestbetrag reduziert\n",
      "wird. Erzeuge dann Terminalknoten mit Mittelwert aus den aktuellen\n",
      "Beispielen in dem Knoten.\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 35\n",
      "Evaluierung von Baumlernern\n",
      "Doch zurück zur Klassiﬁaktion. Die Konfusionsmatrix lautet:\n",
      "tatsächlich\n",
      "Vorhergesagt\n",
      "+\n",
      "Vorhergesagt\n",
      "−\n",
      "+\n",
      "True positives\n",
      "TP\n",
      "False negatives\n",
      "FN\n",
      "Recall:\n",
      "TP/(TP + FN)\n",
      "−\n",
      "False positives\n",
      "FP\n",
      "True negatives\n",
      "TN\n",
      "Precision:\n",
      "TP/(TP + FP)\n",
      "Accuracy: P(ˆf(x) = y) geschätzt als (TP + TN)/total\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 36\n",
      "Balance von FP und FN\n",
      "▶ F-measure: β·recall·precision\n",
      "recall+precision =\n",
      "βTP\n",
      "βTP+FP+FN\n",
      "▶ Verlaufsformen:\n",
      "▶ Lift: TP für verschiedene Stichprobengrößen S\n",
      "schön\n",
      "▶ Receiver Operating Characteristic (ROC): für verschiedene TP jeweils die FP\n",
      "anzeigen\n",
      "schön\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 37\n",
      "ROC genauer\n",
      "▶ Statt der absoluten Anzahl TP nimm die Raten von true oder false positives –\n",
      "ergibt eine glatte Kurve.\n",
      "▶ Für jeden Prozentsatz von falschen Positiven nimm eine Hypothese h, deren\n",
      "Extension diese Anzahl von FP hat und zähle die TP.\n",
      "▶ TPrate := TP/P ∼ recall bezogen auf eine Untermenge\n",
      "▶ FPrate := FP/N ∼ FP/FP + TN bezogen auf Untermenge\n",
      "schön\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 38\n",
      "Kosten von Fehlern\n",
      "▶ Nicht immer sind FP so schlimm wie FN\n",
      "▶ medizinische Anwendungen: lieber ein Alarm zu viel als einen zu wenig!\n",
      "▶ Gewichtung der Beispiele:\n",
      "▶ Wenn FN 3x so schlimm ist wie FP, dann gewichte negative Beispiele 3x höher\n",
      "als positive.\n",
      "▶ Wenn FP 10x so schlimm ist wie FN, dann gewichte positive Beispiele 10x höher\n",
      "als negative.\n",
      "▶ Lerne den Klassiﬁkator mit den gewichteten Beispielen wie üblich. So kann\n",
      "jeder Lerner Kosten berücksichtigen!\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 39\n",
      "Bäume in R — rpart\n",
      "Zum Lernen: Funktion rpart() aus Paket rpart.\n",
      "> l i b r a r y ( \" r p a r t \" )\n",
      "> set . seed(20060911)\n",
      "> pid_rpart <− r p a r t ( diabetes\n",
      "~. ,\n",
      "data = pid_learn )\n",
      "> p l o t ( pid_rpart ,\n",
      "uniform = TRUE)\n",
      "> t e x t ( pid_rpart )\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 40\n",
      "Bäume in R — rpart\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 41\n",
      "Bäume in R — rpart\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 42\n",
      "Bäume in R — rpart\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 43\n",
      "Bäume in R — rpart\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 44\n",
      "Was haben sie kennengelernt?\n",
      "▶ Sie kennen ID3 und CART als Beispiel für TDIDT.\n",
      "▶ Für das Lernen verwendet ID3 das Gütemaß des Informationsgewinns auf\n",
      "Basis der Entropie. CART verwendet den Gini-Index\n",
      "▶ Man kann etwas über die Performanz aussagen:\n",
      "▶ Man kann abschätzen, wie nah das Lernergebnis der unbekannten Wahrheit\n",
      "kommt → Konﬁdenz\n",
      "▶ Man kann abschätzen, wie groß der Fehler sein wird und dies zum Stutzen des\n",
      "gelernten Baums nutzen.\n",
      "▶ Lernergebnisse werden also evaluiert:\n",
      "▶ Einzelwerte: accuracy, precision, recall, F-measure\n",
      "▶ Verläufe: ROC\n",
      "Diese Evaluationsmethoden gelten nicht nur für Entscheidungsbäume!\n",
      "▶ Sie kennen das R package rpart\n",
      "DMML, basierend auf InDaS, BMBF, 01IS17063B | FG AIML, TU Darmstadt | K. Kersting | 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)  # Corrected this line\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"lectures\\\\04_baumbasierteVerfahren.pdf\"\n",
    "pdf_text = extract_text_from_pdf(file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def break_into_sections(text, max_tokens_per_section):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    words = text.split()\n",
    "    sections = []\n",
    "    section = \"\"\n",
    "    token_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        if token_count + len(tokens) < max_tokens_per_section:\n",
    "            section += \" \" + word\n",
    "            token_count += len(tokens)\n",
    "        else:\n",
    "            sections.append(section.strip())\n",
    "            section = word\n",
    "            token_count = len(tokens)\n",
    "    \n",
    "    # Add the last section if it's non-empty\n",
    "    if section:\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Example usage:\n",
    "sections = break_into_sections(pdf_text, max_tokens_per_section=1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized 1 out of 6 sections\n",
      "Current summary: Decision trees are a type of model used in data mining and machine learning. They are based on a series of binary decisions, with each decision leading to the next step in the tree and ultimately a prediction or classification. Decision trees are often used because they are easy to interpret and provide direct explanations for predictions. They can also be learned from data, using simple algorithms, and can be combined with other models. As shown in examples such as predicting the type of material for a production process or classifying soil samples for agriculture, decision trees can be useful for a variety of applications.\n",
      "Summarized 2 out of 6 sections\n",
      "Current summary: 1. The example set is the secondary testing set. 2. Separate the examples with upper density after leaf node using the predetermined threshold (4.5) into S1. 3. Calculate the information gain for the attributes of humidity, acid and temperature. 4. Choose the attribute with minimum error rate. 5. If the attribute is humidity, choose majority results (\"+\") with the number of examples that are wet is 5/16 to be the prediction (better than the \"∀\" on majority prediction rate of 11/16). 6. If the attribute is acid, choose breeding results (\"+\") with (9 - 5 + 8) / 16 > 0 and choose the minority prediction values (\"∀\") at majority prediction with 5/16 value for the set of wet examples because the number of mini prediction examples are greater, is smaller. 7. If the attribute is temperature, choose the majority prediction (\"∀\") with\n",
      "Summarized 3 out of 6 sections\n",
      "Current summary: Top down induction of decision trees (TDIDT) is an algorithm used for machine learning to construct a decision tree based on a set of training data. The algorithm recursively splits the training data based on the selection of the best feature and its values, until all training data belongs to the same class or the tree reaches a certain depth. However, larger trees can lead to overfitting and make interpretation and understanding of the tree difficult. Therefore, pruning is used to reduce the complexity of the tree and prevent overfitting. This is achieved by either replacing a subtree with a single node or pruning branches that do not significantly improve the performance of the tree. Stutzen is important to avoid overfitting and make the tree more interpretable and understandable.\n",
      "Rate limit exceeded. Waiting for 60 seconds...\n",
      "Summarized 4 out of 6 sections\n",
      "Current summary: Stutzen (pruning) is a method for finding the tree of the \"right size\". It involves trimming the branches of the largest tree starting from the end nodes (bottom-up). This is done by estimating the error at each node and taking into account the accuracy of the estimation. This is achieved by determining a confidence interval. If the upper bound of the confidence interval at the parent node is smaller than the sum of the confidence intervals at the child nodes, the child nodes are pruned. To calculate the confidence interval, the data is transformed into a standard normal distribution using the z-transformation. This allows us to calculate the probability that the mean lies within a certain interval. Specifically for binary observations, such as in decision trees, the success rate (p+) is used to calculate the confidence interval. Using this, we can determine the confidence interval for the success rate of each node.\n",
      "Rate limit exceeded. Waiting for 60 seconds...\n",
      "Summarized 5 out of 6 sections\n",
      "Current summary: - Decision trees are a popular machine learning method for classification and regression tasks.\n",
      "- They are constructed by recursively splitting data based on features, in order to minimize a certain criterion.\n",
      "- The most commonly used criteria are the information gain, Gini index, and sum of squared errors.\n",
      "- A split is chosen if it leads to a decrease in the criterion, and the process continues until a stopping criterion is met (such as a certain tree depth or number of data points in a leaf node).\n",
      "- To avoid overfitting, techniques such as pruning and early stopping are used.\n",
      "- Evaluation of decision tree models can be done through metrics such as accuracy, precision, recall, F-measure, lift, and receiver operating characteristic (ROC) curves.\n",
      "- The choice of these metrics depends on the specific task and the relative importance of different types of errors.\n",
      "- In medical applications, the cost of false positives and false negatives may differ, and the appropriate metric must be chosen accordingly.\n",
      "Summarized 6 out of 6 sections\n",
      "Current summary: * We discussed trees as a type of supervised learning algorithm that can be used for both regression and classification tasks.\n",
      "* We looked at two popular tree-based learning algorithms: ID3 and CART, which use different metrics (information gain and Gini index) for selecting the best split at each node.\n",
      "* We learned about the concept of overfitting and how pruning can help prevent it.\n",
      "* We also discussed how to evaluate the performance of a learned tree, using metrics such as accuracy, precision, recall, and F-measure, as well as the ROC curve.\n",
      "* Finally, we saw an example of how to use the rpart package in R to build a decision tree model for a classification task.\n",
      "Decision trees are a type of model used in data mining and machine learning. They are based on a series of binary decisions, with each decision leading to the next step in the tree and ultimately a prediction or classification. Decision trees are often used because they are easy to interpret and provide direct explanations for predictions. They can also be learned from data, using simple algorithms, and can be combined with other models. As shown in examples such as predicting the type of material for a production process or classifying soil samples for agriculture, decision trees can be useful for a variety of applications. 1. The example set is the secondary testing set. 2. Separate the examples with upper density after leaf node using the predetermined threshold (4.5) into S1. 3. Calculate the information gain for the attributes of humidity, acid and temperature. 4. Choose the attribute with minimum error rate. 5. If the attribute is humidity, choose majority results (\"+\") with the number of examples that are wet is 5/16 to be the prediction (better than the \"∀\" on majority prediction rate of 11/16). 6. If the attribute is acid, choose breeding results (\"+\") with (9 - 5 + 8) / 16 > 0 and choose the minority prediction values (\"∀\") at majority prediction with 5/16 value for the set of wet examples because the number of mini prediction examples are greater, is smaller. 7. If the attribute is temperature, choose the majority prediction (\"∀\") with Top down induction of decision trees (TDIDT) is an algorithm used for machine learning to construct a decision tree based on a set of training data. The algorithm recursively splits the training data based on the selection of the best feature and its values, until all training data belongs to the same class or the tree reaches a certain depth. However, larger trees can lead to overfitting and make interpretation and understanding of the tree difficult. Therefore, pruning is used to reduce the complexity of the tree and prevent overfitting. This is achieved by either replacing a subtree with a single node or pruning branches that do not significantly improve the performance of the tree. Stutzen is important to avoid overfitting and make the tree more interpretable and understandable. Stutzen (pruning) is a method for finding the tree of the \"right size\". It involves trimming the branches of the largest tree starting from the end nodes (bottom-up). This is done by estimating the error at each node and taking into account the accuracy of the estimation. This is achieved by determining a confidence interval. If the upper bound of the confidence interval at the parent node is smaller than the sum of the confidence intervals at the child nodes, the child nodes are pruned. To calculate the confidence interval, the data is transformed into a standard normal distribution using the z-transformation. This allows us to calculate the probability that the mean lies within a certain interval. Specifically for binary observations, such as in decision trees, the success rate (p+) is used to calculate the confidence interval. Using this, we can determine the confidence interval for the success rate of each node. - Decision trees are a popular machine learning method for classification and regression tasks.\n",
      "- They are constructed by recursively splitting data based on features, in order to minimize a certain criterion.\n",
      "- The most commonly used criteria are the information gain, Gini index, and sum of squared errors.\n",
      "- A split is chosen if it leads to a decrease in the criterion, and the process continues until a stopping criterion is met (such as a certain tree depth or number of data points in a leaf node).\n",
      "- To avoid overfitting, techniques such as pruning and early stopping are used.\n",
      "- Evaluation of decision tree models can be done through metrics such as accuracy, precision, recall, F-measure, lift, and receiver operating characteristic (ROC) curves.\n",
      "- The choice of these metrics depends on the specific task and the relative importance of different types of errors.\n",
      "- In medical applications, the cost of false positives and false negatives may differ, and the appropriate metric must be chosen accordingly. * We discussed trees as a type of supervised learning algorithm that can be used for both regression and classification tasks.\n",
      "* We looked at two popular tree-based learning algorithms: ID3 and CART, which use different metrics (information gain and Gini index) for selecting the best split at each node.\n",
      "* We learned about the concept of overfitting and how pruning can help prevent it.\n",
      "* We also discussed how to evaluate the performance of a learned tree, using metrics such as accuracy, precision, recall, and F-measure, as well as the ROC curve.\n",
      "* Finally, we saw an example of how to use the rpart package in R to build a decision tree model for a classification task.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def summarize_text(text):\n",
    "    sections = break_into_sections(text, max_tokens_per_section=1700)\n",
    "    summaries = []\n",
    "    \n",
    "    for section in sections:\n",
    "        summary = \"\"\n",
    "        while not summary:\n",
    "            try:\n",
    "                response = openai.Completion.create(\n",
    "                    engine=\"gpt-3.5-turbo-instruct\",\n",
    "                    prompt=f\"{section} \\n\\n This part can be summarized as follows:\",\n",
    "                    max_tokens=200\n",
    "                )\n",
    "                summary = response['choices'][0]['text'].strip()\n",
    "                summaries.append(summary)\n",
    "            except openai.error.OpenAIError as e:\n",
    "                if 'Rate limit reached' in str(e):\n",
    "                    print('Rate limit exceeded. Waiting for 60 seconds...')\n",
    "                    time.sleep(30)  # Wait for 60 seconds before retrying\n",
    "                else:\n",
    "                    print(f'An error occurred: {e}')\n",
    "                    break  # Break out of the loop for other errors\n",
    "        print(f'Summarized {len(summaries)} out of {len(sections)} sections')\n",
    "        print(f'Current summary: {summary}')\n",
    "    \n",
    "    return \" \".join(summaries)\n",
    "\n",
    "final_summary = summarize_text(pdf_text)\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_final(summary):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=f\"{summary} \\n \\n Overall one can summarize the text in the following 10 bullets each having subpoints to elaborate further:\",\n",
    "        max_tokens=600\n",
    "    )\n",
    "    print(response[\"choices\"])\n",
    "    return response['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x1b54a5051d0> JSON: {\n",
      "  \"text\": \" \\n\\n1. Decision trees are a type of model used in data mining and machine learning.\\n    * They are based on a series of binary decisions.\\n    * Each decision leads to the next step in the tree.\\n    * The goal is to make a prediction or classification.\\n2. Decision trees are easy to interpret and provide direct explanations for predictions.\\n    * This makes them useful for a variety of applications.\\n    * Examples include predicting the type of material for a production process or classifying soil samples for agriculture.\\n3. Top-down induction of decision trees (TDIDT) is an algorithm used to construct a decision tree from a set of training data.\\n    * It recursively splits the data based on the best feature and its values.\\n    * The process continues until all data belongs to the same class or a stopping criterion is met.\\n4. Pruning is used to reduce the complexity of the tree and prevent overfitting.\\n    * This can involve replacing a subtree with a single node or pruning branches that do not significantly improve performance.\\n    * It is important for making the tree more interpretable and understandable.\\n5. There are various metrics used to choose the best split at each node, including information gain, Gini index, and sum of squared errors.\\n    * A split is chosen if it leads to a decrease in the chosen metric.\\n    * The choice of metric may vary depending on the task and the types of errors that are most important to minimize.\\n6. Overfitting is a common problem with decision trees and can be mitigated through techniques such as pruning and early stopping.\\n    * Overfitting occurs when the tree is too complex and fits the training data too closely, leading to poor performance on new data.\\n    * Pruning involves trimming the tree to find the \\\"right size\\\" and prevent overfitting.\\n7. Evaluation of decision tree models can be done using metrics such as accuracy, precision, recall, F-measure, lift, and receiver operating characteristic (ROC) curves.\\n    * The choice of metric depends on the specific task and the relative importance of different types of errors.\\n    * In medical applications, the cost of false positives and false negatives may differ, requiring the appropriate metric to be chosen.\\n8. There are different algorithms used to build decision trees, such as ID3 and CART, which use different metrics for selecting the best split at each node.\\n    * ID3 uses information gain as the metric, while CART uses the Gini index.\\n    * These algorithms can be implemented in various programming languages such as R, Python, and Java.\\n9. Decision trees can be used for both regression and classification tasks.\\n    * Regression tasks involve predicting a continuous value, while classification tasks involve predicting a categorical label.\\n    * The choice of metric and stopping criterion may differ between these two types of tasks.\\n10. Decision trees have both advantages and limitations.\\n    * They are easy to interpret and can handle large amounts of data.\\n    *\",\n",
      "  \"index\": 0,\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}]\n",
      "1. Decision trees are a type of model used in data mining and machine learning.\n",
      "    * They are based on a series of binary decisions.\n",
      "    * Each decision leads to the next step in the tree.\n",
      "    * The goal is to make a prediction or classification.\n",
      "2. Decision trees are easy to interpret and provide direct explanations for predictions.\n",
      "    * This makes them useful for a variety of applications.\n",
      "    * Examples include predicting the type of material for a production process or classifying soil samples for agriculture.\n",
      "3. Top-down induction of decision trees (TDIDT) is an algorithm used to construct a decision tree from a set of training data.\n",
      "    * It recursively splits the data based on the best feature and its values.\n",
      "    * The process continues until all data belongs to the same class or a stopping criterion is met.\n",
      "4. Pruning is used to reduce the complexity of the tree and prevent overfitting.\n",
      "    * This can involve replacing a subtree with a single node or pruning branches that do not significantly improve performance.\n",
      "    * It is important for making the tree more interpretable and understandable.\n",
      "5. There are various metrics used to choose the best split at each node, including information gain, Gini index, and sum of squared errors.\n",
      "    * A split is chosen if it leads to a decrease in the chosen metric.\n",
      "    * The choice of metric may vary depending on the task and the types of errors that are most important to minimize.\n",
      "6. Overfitting is a common problem with decision trees and can be mitigated through techniques such as pruning and early stopping.\n",
      "    * Overfitting occurs when the tree is too complex and fits the training data too closely, leading to poor performance on new data.\n",
      "    * Pruning involves trimming the tree to find the \"right size\" and prevent overfitting.\n",
      "7. Evaluation of decision tree models can be done using metrics such as accuracy, precision, recall, F-measure, lift, and receiver operating characteristic (ROC) curves.\n",
      "    * The choice of metric depends on the specific task and the relative importance of different types of errors.\n",
      "    * In medical applications, the cost of false positives and false negatives may differ, requiring the appropriate metric to be chosen.\n",
      "8. There are different algorithms used to build decision trees, such as ID3 and CART, which use different metrics for selecting the best split at each node.\n",
      "    * ID3 uses information gain as the metric, while CART uses the Gini index.\n",
      "    * These algorithms can be implemented in various programming languages such as R, Python, and Java.\n",
      "9. Decision trees can be used for both regression and classification tasks.\n",
      "    * Regression tasks involve predicting a continuous value, while classification tasks involve predicting a categorical label.\n",
      "    * The choice of metric and stopping criterion may differ between these two types of tasks.\n",
      "10. Decision trees have both advantages and limitations.\n",
      "    * They are easy to interpret and can handle large amounts of data.\n",
      "    *\n"
     ]
    }
   ],
   "source": [
    "bullets = summarize_final(final_summary)\n",
    "print(bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 2049 tokens, however you requested 11819 tokens (11519 in your prompt; 300 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\proof_of_concept.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m summary\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m summary \u001b[39m=\u001b[39m summarize_text(pdf_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(summary)\n",
      "\u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\proof_of_concept.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msummarize_text\u001b[39m(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdavinci\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSummarize the following text:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mtext\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m  \u001b[39m# Limit the summary length\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     summary \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/timal/OneDrive%20-%20stud.tu-darmstadt.de/Gladiators/study-assist/proof_of_concept.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m summary\n",
      "File \u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\env\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\env\\lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\env\\lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\timal\\OneDrive - stud.tu-darmstadt.de\\Gladiators\\study-assist\\env\\lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 2049 tokens, however you requested 11819 tokens (11519 in your prompt; 300 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Ensure you've set up your API keys properly\n",
    "openai.api_key = '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'\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=f\"Summarize the following text:\\n{text}\",\n",
    "        max_tokens=300  # Limit the summary length\n",
    "    )\n",
    "    summary = response['choices'][0]['text'].strip()\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "summary = summarize_text(pdf_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
